{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm_joblib import tqdm_joblib\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('asl-signs/train.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = json.load(open('asl-signs/sign_to_prediction_index_map.json'))\n",
    "print(label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_check(df):\n",
    "    min = df['frame'].min()\n",
    "    max = df['frame'].max()\n",
    "    if (max-min) < 5:\n",
    "        return False\n",
    "    else: \n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_count(df):\n",
    "    min = df['frame'].min()\n",
    "    max = df['frame'].max()\n",
    "    return max, min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_file(filename):\n",
    "    # start = timeit.default_timer()\n",
    "    df_tmp = pq.read_table(f'asl-signs/{filename}').to_pandas()\n",
    "    if not frame_check(df_tmp):\n",
    "        return np.array([0]), False\n",
    "    df_tmp = df_tmp.fillna(0)\n",
    "    face_landmarks = [61, 185, 40, 39, 37, 0, 267, 269, 270, 409, \n",
    "                     291, 146, 91, 181, 84, 17, 314, 405, 321, 375,  \n",
    "                     191, 80, 81, 82, 13, 312, 311, 310, 415, 308, \n",
    "                     95, 88, 178, 87, 14, 317, 402, 318, 324]\n",
    "    pose_landmarks = [11, 12, 13, 14, 15, 16,\n",
    "                      17, 18, 19, 20, 21, 22]\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    z_list = []\n",
    "    all_list = []\n",
    "    hand = ['left_hand', 'right_hand']\n",
    "    tmp_frame = 0\n",
    "    max_f, min = frame_count(df_tmp)\n",
    "    frames = max_f-min+1\n",
    "    df_tmp = df_tmp.query('type == @hand or (type == \"face\" & landmark_index == @face_landmarks) or (type == \"pose\" & landmark_index == @pose_landmarks) ')\n",
    "    if frames > 20:\n",
    "        buff = max_f-20\n",
    "        df_tmp = df_tmp.query('frame >= @buff')\n",
    "        for i in range(max_f-19, max_f+1):\n",
    "            tmp_df = df_tmp.query('frame == @i')\n",
    "            all_list.append(np.array([tmp_df['x'].astype(np.float32).to_numpy(), tmp_df['y'].astype(np.float32).to_numpy(), tmp_df['z'].astype(np.float32).to_numpy()]).flatten())\n",
    "    else:\n",
    "        for i in range(min, max_f+1):\n",
    "            tmp_df = df_tmp.query('frame == @i')\n",
    "            all_list.append(np.array([tmp_df['x'].astype(np.float32).to_numpy(), tmp_df['y'].astype(np.float32).to_numpy(), tmp_df['z'].astype(np.float32).to_numpy()]).flatten())\n",
    "    ret = np.array(all_list).shape[0]\n",
    "    if ret < 20:\n",
    "        add_list = [[0 for i in range(279)] for j in range(20-ret)]\n",
    "        all_list = add_list + all_list\n",
    "    # print(timeit.default_timer() - start)\n",
    "    shp = np.array(all_list).shape\n",
    "    try:\n",
    "        if shp[0] != 20 or shp[1]!= 279:\n",
    "            print(np.array(all_list).shape)\n",
    "    except:\n",
    "        return np.array([0]), False       \n",
    "    else:\n",
    "        if shp[0] != 20 or shp[1]!= 279:\n",
    "            print(np.array(all_list).shape)\n",
    "        return np.asarray(all_list), True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_label(raw):\n",
    "    return label_map[raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parallelism\n",
    "\n",
    "path = df['path'].to_numpy().astype(str)\n",
    "raw_label = df['sign'].to_numpy().astype(str)\n",
    "with tqdm_joblib(desc=\"Label conversion\", total=94477) as progress_bar:\n",
    "    label = Parallel(n_jobs=-1)(delayed(convert_label)(i) for i in raw_label)\n",
    "with tqdm_joblib(desc=\"Data conversion\", total=94477) as progress_bar:\n",
    "    data, cond = zip(*Parallel(n_jobs=-1)(delayed(extract_file)(i) for i in path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing false data\n",
    "new_df = pd.DataFrame(data=[data, cond, label]).T\n",
    "new_df.columns = ['data', 'cond', 'label']\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df[new_df.cond != False]\n",
    "new_df = new_df.drop(['cond'], axis=1)\n",
    "new_df = new_df.reset_index(drop=True)\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = [data for data in new_df['data'].to_numpy()]\n",
    "X_data = np.array(X_data)\n",
    "print(X_data.shape)\n",
    "Y_data = [label for label in new_df['label'].to_numpy()]\n",
    "Y_data = np.array(Y_data)\n",
    "Y_data = to_categorical(Y_data).astype(int)\n",
    "print(Y_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"feature_data.npy\", X_data)\n",
    "np.save(\"label_data.npy\", Y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_data = np.load(\"feature_data.npy\")\n",
    "Y_data = np.load(\"label_data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X, y_train, y = train_test_split(X_data, Y_data, test_size=0.1, shuffle=True)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X, y, test_size=0.5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Flatten, GRU, Conv1D\n",
    "from tensorflow.keras.callbacks import TensorBoard, Callback\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class trainingCallback(Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    \n",
    "    # Check accuracy\n",
    "    # if(logs.get('categorical_accuracy') < 0.95  and logs.get('loss') < 0.35 and logs.get('val_loss') < 0.35):\n",
    "    if((logs.get('categorical_accuracy') > 0.95) or (logs.get('categorical_accuracy') > 0.92  and logs.get('loss') > logs.get('val_loss'))):\n",
    "      # Stop if threshold is met\n",
    "      print(\"\\nAccuracy grater than 0.92 so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "\n",
    "# Instantiate class\n",
    "callbacks = trainingCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "inputs = tf.keras.layers.Input(shape=(20,279))\n",
    "x_1, w, h = tf.keras.layers.LSTM(256, return_sequences=False, activation='relu', return_state=True)(inputs)\n",
    "x = tf.keras.layers.Dropout(0.2)(x_1)\n",
    "# x = tf.keras.layers.LSTM(128, return_sequences=False, activation='relu')(x, initial_state=[w, h])\n",
    "concat = tf.keras.layers.concatenate([x, w, h])\n",
    "# flatten = tf.keras.layers.Flatten(concat)\n",
    "x = tf.keras.layers.Dense(512, activation='relu')(concat)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "# x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "# x = tf.keras.layers.Dropout(0.5)(x)\n",
    "out = tf.keras.layers.Dense(250, activation='softmax', name='outputs')(x)\n",
    "model_LSTM = tf.keras.Model(inputs, out)\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM.compile(optimizer=Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train = model_LSTM.fit(X_train, y_train, epochs=200, batch_size=64,validation_data=(X_val,y_val), callbacks=[callbacks])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model\n",
    "inputs = tf.keras.layers.Input(shape=(20,279), name='inputs')\n",
    "x_1= tf.keras.layers.LSTM(256, return_sequences=True, activation='relu', name='lstm_1')(inputs)\n",
    "x = tf.keras.layers.Dropout(0.2, name='drop_1')(x_1)\n",
    "x, w, h= tf.keras.layers.LSTM(512, return_sequences=False, activation='relu', return_state=True, name='lstm_2')(x)\n",
    "# x = tf.keras.layers.LSTM(128, return_sequences=False, activation='relu')(x, initial_state=[w, h])\n",
    "concat = tf.keras.layers.concatenate([x, w, h], name='concat_1')\n",
    "# flatten = tf.keras.layers.Flatten(concat)\n",
    "x = tf.keras.layers.Dense(1024, activation='relu', name='dense_1')(concat)\n",
    "x = tf.keras.layers.Dropout(0.2, name='drop_2')(x)\n",
    "# x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "# x = tf.keras.layers.Dropout(0.5)(x)\n",
    "out = tf.keras.layers.Dense(250, activation='softmax', name='outputs')(x)\n",
    "model_LSTM_2 = tf.keras.Model(inputs, out)\n",
    "model_LSTM_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_LSTM_2.compile(optimizer=Adam(learning_rate=1e-3), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "model_train = model_LSTM_2.fit(X_train, y_train, epochs=10, batch_size=64,validation_data=(X_val,y_val), callbacks=[callbacks])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
